name: Daily Automated Scraping

# Run daily at 3:00 AM UTC
on:
  schedule:
    - cron: '0 3 * * *'  # 3:00 AM UTC every day

  # Allow manual trigger for testing
  workflow_dispatch:

# Only allow one scraping job at a time
concurrency:
  group: scraping
  cancel-in-progress: false  # Don't cancel running scraping jobs

jobs:
  scrape-and-update:
    name: Scrape Data and Update PostgreSQL
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Scraping typically takes ~12 minutes

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip'  # Cache pip dependencies for faster runs

      - name: Install Firefox and geckodriver
        run: |
          # Remove snap Firefox if present
          sudo snap remove firefox || true

          # Install Firefox from Mozilla PPA (traditional apt installation)
          sudo add-apt-repository -y ppa:mozillateam/ppa
          sudo apt-get update

          # Set Mozilla PPA priority higher than snap
          echo '
          Package: *
          Pin: release o=LP-PPA-mozillateam
          Pin-Priority: 1001
          ' | sudo tee /etc/apt/preferences.d/mozilla-firefox

          # Install Firefox
          sudo apt-get install -y firefox

          # Install geckodriver
          GECKODRIVER_VERSION=v0.34.0
          wget -q https://github.com/mozilla/geckodriver/releases/download/$GECKODRIVER_VERSION/geckodriver-$GECKODRIVER_VERSION-linux64.tar.gz
          tar -xzf geckodriver-$GECKODRIVER_VERSION-linux64.tar.gz
          sudo mv geckodriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/geckodriver

          # Verify installation
          which firefox
          firefox --version
          geckodriver --version

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r server/requirements.txt

      - name: Run scraping and database update
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          PATH: /snap/bin:/usr/local/bin:/usr/bin:/bin
        run: |
          # Create empty .env file for pydantic-settings compatibility
          touch .env
          touch server/.env

          # Debug: Print DATABASE_URL length (not content for security)
          echo "DATABASE_URL length: ${#DATABASE_URL}"

          # Verify Firefox is accessible
          which firefox
          firefox --version

          python server/scripts/scrape_and_update.py

      - name: Report success
        if: success()
        run: |
          echo "✅ Scraping completed successfully"
          echo "Timestamp: $(date -u +'%Y-%m-%d %H:%M:%S UTC')"

      - name: Report failure
        if: failure()
        run: |
          echo "❌ Scraping failed"
          echo "Check logs above for details"
          echo "Discord notification should have been sent if DISCORD_WEBHOOK_URL is configured"
